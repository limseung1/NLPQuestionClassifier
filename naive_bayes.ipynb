{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('nps_chat')\n",
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features[f'contains({word.lower()})'] = True\n",
    "    return features\n",
    "\n",
    "def generate_binary_feature(label):\n",
    "    return label in ['whQuestion', 'yAnswer','ynQuestion']\n",
    "\n",
    "featuresets = [(dialogue_act_features(post.text), generate_binary_feature(post.get('class'))) for post in posts]\n",
    "\n",
    "# 10% of the total data\n",
    "size = int(len(featuresets) * 0.2)\n",
    "\n",
    "# first 10% for test_set to check the accuracy, and rest 90% after the first 10% for training\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "\n",
    "# get the classifier from the training set\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "\n",
    "# to check the accuracy\n",
    "print(\"Accuracy:\", nltk.classify.accuracy(classifier, test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, classify a new sentence\n",
    "sentences = [\n",
    "    \"Can I ask what you meant at 5:30\",\n",
    "    \"Do you know the answer to that\",\n",
    "    \"Shouldn't it be the case we have answer for this\",\n",
    "    \"Does it make sense that it comes to this\",\n",
    "    \"How long does it take to get the code set up\",\n",
    "    \"Where can I find the rest of the series\",\n",
    "    \"I dont understand why this should work\",\n",
    "    \"I am a little confused at the end what the creator was saying\",\n",
    "    \n",
    "    \"Thank you\",\n",
    "    \"Thank you for creating this video\",\n",
    "    \"By far this is the best video on yt\",\n",
    "    \"This video sucks\",\n",
    "    \"What a joke\",\n",
    "    \"I never liked this\",\n",
    "    \"Super helpful\",\n",
    "    \"I have been teaching all my life and this video does better than what I have done\",\n",
    "]\n",
    "for sentence in sentences:\n",
    "    features = dialogue_act_features(sentence)\n",
    "    predicted_label = classifier.classify(features)\n",
    "    print(\"Sentence: \", sentence)\n",
    "    print(\"Predicted label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with minimum frequency 4: 0.862\n",
      "Vocabulary size before filtering: 5644\n",
      "Vocabulary size after filtering (freq >= 4): 1331\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_word_frequencies(posts):\n",
    "    \"\"\"Count word frequencies across all posts.\"\"\"\n",
    "    word_freq = Counter()\n",
    "    for post in posts:\n",
    "        words = nltk.word_tokenize(post.text.lower())\n",
    "        word_freq.update(words)\n",
    "    return word_freq\n",
    "\n",
    "def dialogue_act_features(post, word_freq, min_freq):\n",
    "    \"\"\"Extract features from post, excluding rare words.\"\"\"\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post.text):\n",
    "        word_lower = word.lower()\n",
    "        if word_freq[word_lower] >= min_freq:\n",
    "            features[f'contains({word_lower})'] = True\n",
    "    return features\n",
    "\n",
    "def generate_binary_feature(label):\n",
    "    \"\"\"Generate binary classification for questions.\"\"\"\n",
    "    return label in ['whQuestion', 'yAnswer', 'ynQuestion']\n",
    "\n",
    "def train_question_classifier(min_freq=1):\n",
    "    # Calculate word frequencies across all posts\n",
    "    word_freq = get_word_frequencies(posts)\n",
    "    \n",
    "    # Generate feature sets with frequency filtering\n",
    "    featuresets = [\n",
    "        (dialogue_act_features(post, word_freq, min_freq),\n",
    "         generate_binary_feature(post.get('class')))\n",
    "        for post in posts\n",
    "    ]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    size = int(len(featuresets) * 0.2)\n",
    "    train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "    \n",
    "    # Train the classifier\n",
    "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "    \n",
    "    # Calculate and return accuracy\n",
    "    accuracy = nltk.classify.accuracy(classifier, test_set)\n",
    "    \n",
    "    return classifier, accuracy, word_freq\n",
    "\n",
    "# Example usage\n",
    "min_freq = 4  # Remove words that appear less than 5 times\n",
    "classifier, accuracy, word_freq = train_question_classifier(min_freq)\n",
    "print(f\"Accuracy with minimum frequency {min_freq}:\", accuracy)\n",
    "\n",
    "# Print vocabulary size before and after filtering\n",
    "total_vocab = len(word_freq)\n",
    "filtered_vocab = len([word for word, freq in word_freq.items() if freq >= min_freq])\n",
    "print(f\"Vocabulary size before filtering: {total_vocab}\")\n",
    "print(f\"Vocabulary size after filtering (freq >= {min_freq}): {filtered_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  Can I ask what you meant at 5:30\n",
      "Predicted label: True\n",
      "Sentence:  Do you know the answer to that\n",
      "Predicted label: True\n",
      "Sentence:  Shouldn't it be the case we have answer for this\n",
      "Predicted label: True\n",
      "Sentence:  Does it make sense that it comes to this\n",
      "Predicted label: True\n",
      "Sentence:  How long does it take to get the code set up\n",
      "Predicted label: True\n",
      "Sentence:  Where can I find the rest of the series\n",
      "Predicted label: True\n",
      "Sentence:  I dont understand why this should work\n",
      "Predicted label: True\n",
      "Sentence:  I am a little confused at the end what the creator was saying\n",
      "Predicted label: True\n",
      "Sentence:  Thank you\n",
      "Predicted label: False\n",
      "Sentence:  Thank you for creating this video\n",
      "Predicted label: False\n",
      "Sentence:  By far this is the best video on yt\n",
      "Predicted label: False\n",
      "Sentence:  This video sucks\n",
      "Predicted label: False\n",
      "Sentence:  What a joke\n",
      "Predicted label: True\n",
      "Sentence:  I never liked this\n",
      "Predicted label: False\n",
      "Sentence:  Super helpful\n",
      "Predicted label: False\n",
      "Sentence:  I have been teaching all my life and this video does better than what I have done\n",
      "Predicted label: True\n"
     ]
    }
   ],
   "source": [
    "# Now, classify a new sentence\n",
    "\n",
    "# Need this to keep original feature set up when testing\n",
    "def dialogue_act_features(post):\n",
    "    features = {}\n",
    "    for word in nltk.word_tokenize(post):\n",
    "        features[f'contains({word.lower()})'] = True\n",
    "    return features\n",
    "\n",
    "sentences = [\n",
    "    \"Can I ask what you meant at 5:30\",\n",
    "    \"Do you know the answer to that\",\n",
    "    \"Shouldn't it be the case we have answer for this\",\n",
    "    \"Does it make sense that it comes to this\",\n",
    "    \"How long does it take to get the code set up\",\n",
    "    \"Where can I find the rest of the series\",\n",
    "    \"I dont understand why this should work\",\n",
    "    \"I am a little confused at the end what the creator was saying\",\n",
    "    \n",
    "    \"Thank you\",\n",
    "    \"Thank you for creating this video\",\n",
    "    \"By far this is the best video on yt\",\n",
    "    \"This video sucks\",\n",
    "    \"What a joke\",\n",
    "    \"I never liked this\",\n",
    "    \"Super helpful\",\n",
    "    \"I have been teaching all my life and this video does better than what I have done\",\n",
    "]\n",
    "for sentence in sentences:\n",
    "    features = dialogue_act_features(sentence)\n",
    "    predicted_label = classifier.classify(features)\n",
    "    print(\"Sentence: \", sentence)\n",
    "    print(\"Predicted label:\", predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package nps_chat to\n",
      "[nltk_data]     /Users/seungwonlim/nltk_data...\n",
      "[nltk_data]   Package nps_chat is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained with accuracy: 0.90\n",
      "\n",
      "Most important features by IDF score:\n",
      "20suser196: 8.889\n",
      "20suser219: 8.889\n",
      "oi: 8.889\n",
      "rolling: 8.889\n",
      "sissy_76: 8.889\n",
      "teensuser7: 8.889\n",
      "40suser0: 8.601\n",
      "blah: 8.601\n",
      "die: 8.601\n",
      "dum: 8.601\n",
      "\n",
      "Classifying example sentences:\n",
      "\n",
      "Sentence: What time is it?\n",
      "Is question: False\n",
      "Confidence: 0.67\n",
      "Top features:\n",
      "  - time: 0.614\n",
      "  - what: 0.492\n",
      "  - it: 0.441\n",
      "\n",
      "Sentence: How does this work?\n",
      "Is question: True\n",
      "Confidence: 0.53\n",
      "Top features:\n",
      "  - does: 0.545\n",
      "  - work: 0.542\n",
      "  - this: 0.473\n",
      "\n",
      "Sentence: This is a statement.\n",
      "Is question: False\n",
      "Confidence: 0.85\n",
      "Top features:\n",
      "  - this: 0.799\n",
      "  - is: 0.602\n",
      "\n",
      "Sentence: Can you help me?\n",
      "Is question: False\n",
      "Confidence: 0.72\n",
      "Top features:\n",
      "  - help: 0.698\n",
      "  - can: 0.490\n",
      "  - me: 0.392\n",
      "\n",
      "Sentence: The weather is nice today.\n",
      "Is question: False\n",
      "Confidence: 0.83\n",
      "Top features:\n",
      "  - weather: 0.587\n",
      "  - today: 0.500\n",
      "  - nice: 0.471\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Download required data\n",
    "nltk.download('nps_chat')\n",
    "posts = nltk.corpus.nps_chat.xml_posts()[:10000]\n",
    "\n",
    "class TfidfQuestionClassifier:\n",
    "    def __init__(self, max_features=1000, min_df=2):\n",
    "        \"\"\"\n",
    "        Initialize the TF-IDF based question classifier\n",
    "        \n",
    "        Parameters:\n",
    "        - max_features: Maximum number of features to use (top words by tf-idf)\n",
    "        - min_df: Minimum document frequency for a word to be included\n",
    "        \"\"\"\n",
    "        self.pipeline = Pipeline([\n",
    "            ('tfidf', TfidfVectorizer(\n",
    "                max_features=max_features,\n",
    "                min_df=min_df,\n",
    "                lowercase=True,\n",
    "                strip_accents='unicode',\n",
    "                analyzer='word'\n",
    "            )),\n",
    "            ('classifier', MultinomialNB())\n",
    "        ])\n",
    "        \n",
    "        # Prepare training data\n",
    "        self.train_classifier()\n",
    "        \n",
    "    def train_classifier(self):\n",
    "        \"\"\"Train the classifier on the NPS chat corpus\"\"\"\n",
    "        # Extract texts and labels\n",
    "        texts = [post.text for post in posts]\n",
    "        labels = [self.is_question(post.get('class')) for post in posts]\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        split_idx = int(len(texts) * 0.2)\n",
    "        train_texts = texts[split_idx:]\n",
    "        train_labels = labels[split_idx:]\n",
    "        test_texts = texts[:split_idx]\n",
    "        test_labels = labels[:split_idx]\n",
    "        \n",
    "        # Train the pipeline\n",
    "        self.pipeline.fit(train_texts, train_labels)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        self.accuracy = self.pipeline.score(test_texts, test_labels)\n",
    "        print(f\"Classifier trained with accuracy: {self.accuracy:.2f}\")\n",
    "        \n",
    "        # Store vocabulary for feature inspection\n",
    "        self.vocabulary = self.pipeline.named_steps['tfidf'].vocabulary_\n",
    "        \n",
    "    @staticmethod\n",
    "    def is_question(label):\n",
    "        \"\"\"Convert NPS chat labels to binary question/non-question\"\"\"\n",
    "        return label in ['whQuestion', 'yAnswer', 'ynQuestion']\n",
    "    \n",
    "    def classify_sentence(self, sentence):\n",
    "        \"\"\"\n",
    "        Classify a single sentence\n",
    "        \n",
    "        Returns a dictionary with classification results and feature importance\n",
    "        \"\"\"\n",
    "        # Get prediction and probability\n",
    "        is_question = self.pipeline.predict([sentence])[0]\n",
    "        proba = self.pipeline.predict_proba([sentence])[0]\n",
    "        \n",
    "        # Get feature importance\n",
    "        tfidf_matrix = self.pipeline.named_steps['tfidf'].transform([sentence])\n",
    "        feature_names = self.pipeline.named_steps['tfidf'].get_feature_names_out()\n",
    "        \n",
    "        # Get non-zero features and their values\n",
    "        nonzero_indices = tfidf_matrix.nonzero()[1]\n",
    "        tfidf_scores = zip(nonzero_indices, tfidf_matrix.data)\n",
    "        important_features = [\n",
    "            (feature_names[idx], score)\n",
    "            for idx, score in sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            'is_question': bool(is_question),\n",
    "            'probability': float(max(proba)),  # Probability of predicted class\n",
    "            'important_features': important_features\n",
    "        }\n",
    "    \n",
    "    def get_most_important_features(self, n=10):\n",
    "        \"\"\"Return the n most important features based on their IDF scores\"\"\"\n",
    "        idf = self.pipeline.named_steps['tfidf'].idf_\n",
    "        feature_names = self.pipeline.named_steps['tfidf'].get_feature_names_out()\n",
    "        \n",
    "        # Sort features by IDF score\n",
    "        important_features = sorted(zip(feature_names, idf), key=lambda x: x[1], reverse=True)\n",
    "        return important_features[:n]\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize classifier\n",
    "    classifier = TfidfQuestionClassifier(max_features=1000, min_df=2)\n",
    "    \n",
    "    # Print most important features overall\n",
    "    print(\"\\nMost important features by IDF score:\")\n",
    "    for feature, score in classifier.get_most_important_features(10):\n",
    "        print(f\"{feature}: {score:.3f}\")\n",
    "    \n",
    "    # Test some example sentences\n",
    "    test_sentences = [\n",
    "        \"What time is it?\",\n",
    "        \"How does this work?\",\n",
    "        \"This is a statement.\",\n",
    "        \"Can you help me?\",\n",
    "        \"The weather is nice today.\",\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nClassifying example sentences:\")\n",
    "    for sentence in test_sentences:\n",
    "        result = classifier.classify_sentence(sentence)\n",
    "        print(f\"\\nSentence: {sentence}\")\n",
    "        print(f\"Is question: {result['is_question']}\")\n",
    "        print(f\"Confidence: {result['probability']:.2f}\")\n",
    "        print(\"Top features:\")\n",
    "        for feature, score in result['important_features'][:3]:\n",
    "            print(f\"  - {feature}: {score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier trained with accuracy: 0.90\n",
      "False\n",
      "0.8059074244288816\n",
      "[('question', np.float64(0.7049749674369392)), ('this', np.float64(0.5665088511428753)), ('is', np.float64(0.42670600753219534))]\n"
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "classifier = TfidfQuestionClassifier(max_features=1000, min_df=2)\n",
    "\n",
    "# Classify a single sentence\n",
    "result = classifier.classify_sentence(\"Is this a question?\")\n",
    "print(result['is_question'])  # True/False\n",
    "print(result['probability'])  # Confidence score\n",
    "print(result['important_features'])  # Most important words\n",
    "\n",
    "# See globally important features\n",
    "important_features = classifier.get_most_important_features(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking content of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Ensure required NLTK data is downloaded\n",
    "nltk.download('nps_chat')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load the NPS chat dataset\n",
    "posts = nltk.corpus.nps_chat.xml_posts()[:100000]\n",
    "\n",
    "# Filter out the posts that are classified as questions\n",
    "question_posts = [post for post in posts if post.get('class') in ['whQuestion', 'ynQuestion']]\n",
    "\n",
    "# Print the total number of question posts\n",
    "print(f\"Total question posts: {len(question_posts)}\")\n",
    "\n",
    "# Optionally, view some of the question posts\n",
    "for post in question_posts[:100]:\n",
    "    print(post.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[5].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts[0].get('class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
