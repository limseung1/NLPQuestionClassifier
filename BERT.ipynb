{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question Classifier\n",
    "\n",
    "# https://huggingface.co/shahrukhx01/question-vs-statement-classifier/blob/main/README.md\n",
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seungwonlim/Desktop/NLPQuestionClassifier/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/seungwonlim/Desktop/NLPQuestionClassifier/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/question-vs-statement-classifier\")\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/question-vs-statement-classifier\")\n",
    "\n",
    "def classify_sentence(sentence, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Classify a sentence as either a question or statement.\n",
    "    \n",
    "    Args:\n",
    "        sentence (str): The input sentence to classify\n",
    "        tokenizer: The loaded tokenizer\n",
    "        model: The loaded model\n",
    "        \n",
    "    Returns:\n",
    "        dict: Classification result with label and confidence score\n",
    "    \"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(sentence, return_tensors=\"tf\", truncation=True, max_length=128)\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(inputs)\n",
    "    predictions = tf.nn.softmax(outputs.logits, axis=-1)\n",
    "    \n",
    "    # Get the predicted class and confidence score\n",
    "    predicted_class = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "    confidence = float(predictions[0][predicted_class])\n",
    "    \n",
    "    # Map class index to label\n",
    "    labels = {0: \"statement\", 1: \"question\"}\n",
    "    predicted_label = labels[predicted_class]\n",
    "    \n",
    "    return {\n",
    "        \"sentence\": sentence,\n",
    "        \"label\": predicted_label,\n",
    "        \"confidence\": round(confidence * 100, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"I don't really understand what you were saying at 5:30\", 'label': 'statement', 'confidence': 99.96}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I don't really understand what you were saying at 5:30\"\n",
    "result = classify_sentence(sentence, tokenizer, model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentence': \"Why can't I find the result\", 'label': 'question', 'confidence': 99.93}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Why can't I find the result\"\n",
    "result = classify_sentence(sentence, tokenizer, model)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "\n",
    "class EmotionDetector:\n",
    "    def __init__(self, model_name: str = \"bhadresh-savani/bert-base-go-emotion\"):\n",
    "        \"\"\"\n",
    "        Initialize emotion detector with a specific BERT model.\n",
    "        Default model includes confusion-like states (e.g., uncertainty)\n",
    "        \"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.model.eval()\n",
    "        \n",
    "    def detect_emotion(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Detect emotions in the given text.\n",
    "        Returns dictionary of emotion scores.\n",
    "        \"\"\"\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            scores = torch.nn.functional.softmax(outputs.logits, dim=1)\n",
    "            \n",
    "        # Get emotion labels and scores\n",
    "        emotions = self.model.config.id2label\n",
    "        emotion_scores = {emotions[i]: score.item() for i, score in enumerate(scores[0])}\n",
    "        \n",
    "        return emotion_scores\n",
    "\n",
    "def analyze_confusion(text: str, threshold: float = 0.3) -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Analyze text specifically for confusion-related emotions\n",
    "    \"\"\"\n",
    "    # Initialize with go-emotion model which includes confusion-like states\n",
    "    detector = EmotionDetector()\n",
    "    \n",
    "    # Get emotion scores\n",
    "    emotions = detector.detect_emotion(text)\n",
    "    \n",
    "    # Focus on confusion-related emotions\n",
    "    confusion_indicators = {\n",
    "        'confusion': emotions.get('confusion', 0),\n",
    "        'uncertainty': emotions.get('uncertainty', 0),\n",
    "        'nervousness': emotions.get('nervousness', 0)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'is_confused': any(score > threshold for score in confusion_indicators.values()),\n",
    "        'confusion_scores': confusion_indicators,\n",
    "        'all_emotions': emotions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: I'm not sure I understand what's going on here.\n",
      "Is confused: True\n",
      "Confusion scores: {'confusion': 0.6531734466552734, 'uncertainty': 0, 'nervousness': 0.0015624117804691195}\n",
      "\n",
      "Text: This doesn't make any sense to me.\n",
      "Is confused: False\n",
      "Confusion scores: {'confusion': 0.010463634505867958, 'uncertainty': 0, 'nervousness': 0.0021200987976044416}\n",
      "\n",
      "Text: I'm completely lost with these instructions.\n",
      "Is confused: False\n",
      "Confusion scores: {'confusion': 0.003107678610831499, 'uncertainty': 0, 'nervousness': 0.015677016228437424}\n",
      "\n",
      "Text: This is crystal clear to me.\n",
      "Is confused: False\n",
      "Confusion scores: {'confusion': 0.0039923000149428844, 'uncertainty': 0, 'nervousness': 0.020075522363185883}\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "example_texts = [\n",
    "    \"I'm not sure I understand what's going on here.\",\n",
    "    \"This doesn't make any sense to me.\",\n",
    "    \"I'm completely lost with these instructions.\",\n",
    "    \"This is crystal clear to me.\"\n",
    "]\n",
    "\n",
    "for text in example_texts:\n",
    "    results = analyze_confusion(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Is confused: {results['is_confused']}\")\n",
    "    print(f\"Confusion scores: {results['confusion_scores']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
